\section{Introduction}

% GANs
Since their introduction in 2014, Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} have gained a lot of popularity in the machine learning community and more specifically in the field of generative models. The GAN framework has been applied with admirable results to many different applications such as computer vision \cite{DBLP:journals/corr/RadfordMC15} \cite{ledig2016photo} \cite{isola2017image}, speech synthesis \cite{kaneko2017generative} \cite{pascual2017segan}, or drug discovery \cite{guimaraes2017objective}. 

We give a brief overview of the GAN framework. GANs consist of two major components, namely a generator and a discriminator, both consisting of a neural network. The aim of the generator is to generate samples that mimic a real data distribution from noise, and the aim of the discriminator is to distinguish samples that come from the real data distribution from the ones produced by the generator. Trained consecutively as an adversarial pair, these networks will enter a game in which the generator is encouraged to model the real data distribution as closely as possible with the discriminator acting as a critic as accurate as possible. On top of their practical abilities, this makes GANs very interesting for their inherent theoretical aspects, paving the way to leveraging methods and techniques that were formerly used in game theory exclusively.

Now, the training of GANs is an infamously intricate task. In fact, a significant amount of the research on GANs has been focusing on how to improve the stability of their training. More specifically, the performance of the discriminator gets hard to control since the density ratio estimation in high-dimensional spaces is commonly inaccurate and unstable; the generator can then hardly learn the multimodal structure of the target distribution.


% SN
There has been numerous efforts to overcome this stability issue, amongst which expressing the network's objective functions differently, i.e. re-writing the discriminator and generator losses was shown promising. For instance, a different probability distribution divergence measure is proposed with the Wasserstein GAN (WGAN) \cite{arjovsky2017wasserstein}. Another example is the Least Squares GAN (LSGAN), where the vanishing gradient problem of the sigmoid function used in the original GAN is exposed and a solution is proposed by adopting a least squares loss function for the discriminator loss.  

In a different approach, this time more architectural, spectral normalization proposes a method that reduces the function space from which can originate the discriminator to the space of Lipschitz continuous functions, assuring the boundedness of statistics \todo{cite}. This is done by normalizing the weight matrix of each layer using their spectral norm. The authors show that doing so not only improves the stability of the learning, but also that the method is easily implemented and tuned. We describe spectral normalization in more details in Section \ref{sec:bg-sn}. This recent development has received a hugely positive appraisal and is seen by many as a major breakthrough in deep learning.

% Problem statement
\todo{problem statement} In this work, we 


% Expectations
\todo{expectations} 


\begin{enumerate}
	\item GANs
	\item SN and the huge reach it had
	\item What we're going to do / problem statement: evaluate SN on different GANs using the IS on two datasets
	\item Expectations
\end{enumerate}
Motivate the problem you are trying to solve, attempt to make an intuitive description of the problem and also formally define the problem. (1-2 pages including title, authors and abstract)

%TODO
%Motivate the problem

%In this project, we will implement a number of different Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} for image generation.
 
%We plan to at least implement DCGAN \cite{DBLP:journals/corr/RadfordMC15} with the new Spectral Normalization \cite{miyato2018spectral}. On top of that and if time allows, we will also implement different losses such as LSGAN \cite{mao2017least} or WGAN \cite{arjovsky2017wasserstein}, and various training improvement techniques such as mini-batch discrimination \cite{salimans2016improved}. 

%For evaluating the performance of these GANs, we will implement the inception score metric as described in \cite{salimans2016improved}. We already have made up a dataset of 30K animal pictures (mostly reptiles), fetched from the Flickr API. If that turns out to be too few or not suitable for any reason we will fall back to using CIFAR-100 or ImageNet (or a subset of these).

The purpose of this project is to investigate the performance of the different types of Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} for image generation as well as possible improvement options.
The project was originally defined based on varying levels of priority where everything assigned priority 1 was promised to be completed.

\begin{itemize}
	\item Implement Deep Convolutional Generative Adversial Network with original loss \cite{DBLP:journals/corr/RadfordMC15} (priority 1)
	\item Implement the inception score metric \cite{salimans2016improved} (priority 1)
	\item Implement Spectral Normalization (priority 1)
	\item Evaluate all our GANs on our reptile dataset (priority 1)
	\item Implement other losses (LSGAN, WGAN) \cite{mao2017least} (priority 2)
	\item Evaluate GANs on CIFAR-100 (priority 2)
	\item Implement mini-batch discrimination and or other improvements \cite{salimans2016improved}. (priority 3)
\end{itemize}

In order to evaluate the performance of these GANs, the evaluation metric known as the inception score, as described in \cite{salimans2016improved}, will be implemented. Furthermore a data set consisting of roughly 30K animal pictures (mostly reptiles), was fetched from the Flickr API while other well known options such as a subset of CIFAR-100 or ImageNet were thought of as possible replacements in the case of unsatisfactory results.

